---
title: "SGE Accounting File"
author: Henrik Bengtsson
date: 2021-06-18
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{SGE Accounting File}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

The SGE accounting file holds information on all submitted and _finished_ jobs submitted to the SGE scheduler.  Queued or currently running jobs are _not_ included in this file.  A job is considered "finished" if it completed successfully, terminated due to an error, or was cancelled.  The file is a colon-delimited text file with one job entry per row, with the most recently finished job appended at end.  Contrary to what one might expect, the file is not _perfectly_ ordered by the `end_time` of the jobs.  We might find some entries where the `end_time` of two consequentive entries might differ by a few seconds in the "wrong" order.  It's not clear to me why this is but it might be that the scheduler updates the SGE accounting file at regular intervals, say every few minutes, and when it does it goes through the jobs in order of job index.


## Indexing the SGE accounting file

In March 2020, it was ~12 GB and took 6-8 minutes to read in full.  As of June 2021, the Wynton HPC accounting file is ~54 GB with 160.5 million job entries.  It is rare to be interested in all entries.  It is more common to work with a subset of the job entries.  In order to do this efficiently, we start by indexing the SGE accounting file to identify the file byte offset for each job entry;

```r
library(wyntonquery)
progressr::handlers(global = TRUE) ## Report on progress
progressr::handlers("progress")

pathname <- sge_accounting_file()
cat(sprintf("File size: %.3g GB\n", file.size(pathname)/1024^3))
# File size: 54.1 GB

## It takes 5-10 minutes to index a 54 GB SGE accounting file
index <- make_file_index(pathname, skip = 4L)
cat(sprintf("Number of job entries: %d\n", length(index)))
# Number of job entries: 160835061

## Save per-job index to file
save_file_index(index, file = "accounting.index_by_row")
```

Next, we want to group the job entries by the ISO-6801 week of the job end times.

```r
library(wyntonquery)
progressr::handlers(global = TRUE) ## Report on progress
progressr::handlers("progress")

pathname <- sge_accounting_file()
index <- read_file_index("accounting.index_by_row")
week_index <- sge_make_week_index(pathname, index = index)
saveRDS(week_index, file = "accounting.index_by_week.rds")

print(week_index)
# A tibble: 201 x 3
   week    nbr_of_jobs file_offset
   <chr>         <dbl>       <dbl>
 1 2017W33         406          59
 2 2017W34         450      113195
 3 2017W35         499      127727
 4 2017W36         523      143361
 5 2017W37         531      150740
 6 2017W38         561      153559
 7 2017W39         579      163657
 8 2017W40         601      169804
 9 2017W41         603      177075
10 2017W42         621      177695
# … with 191 more rows
```



## Reading job entries in SGE accounting file

Here is an example how we can read the job entries for a couple of weeks:

```r
library(wyntonquery)
library(dplyr)

pathname <- sge_accounting_file()
week_index <- readRDS("accounting.index_by_week.rds")

weeks <- subset(week_index, week %in% c("2020W06", "2020W07"))
print(weeks)
# # A tibble: 2 x 3
#   week    nbr_of_jobs file_offset
#   <chr>         <dbl>       <dbl>
# 1 2020W06    30504788 11015936052
# 2 2020W07    31168272 11229385140

offset <- weeks$file_offset[1]
n_max <- sum(weeks$nbr_of_jobs)
jobs <- read_sge_accounting(pathname, offset = offset, n_max = n_max)
jobs <- anonymize(jobs)
print(select(jobs, -account))
# A tibble: 130,010 x 44
   qname  hostname group owner job_name  job_number priority submission_time    
   <chr>  <chr>    <chr> <chr> <chr>          <int>    <int> <dttm>             
 1 long.q cc-id3   grou… owne… job_qb3.…        361       19 2017-10-24 09:33:46
 2 long.q cin-id2  grou… owne… job_qb3.…        360       19 2017-10-23 16:09:59
 3 long.q qb3-hmi… grou… owne… job_qb3.…        365       19 2017-10-24 12:01:17
 4 long.q qb3-id3  grou… owne… job_mac_…        359       19 2017-10-23 09:52:44
 5 long.q qb3-id2  grou… owne… job_qb3.…        363       19 2017-10-24 09:41:57
 6 long.q qb3-id1  grou… owne… bmle_int…        368       19 2017-10-25 09:21:41
 7 long.q cc-hmid1 grou… owne… bmle_int…        369       19 2017-10-25 09:23:47
 8 short… cin-hmi… grou… owne… job_qb3.…        379       10 2017-10-25 10:41:51
 9 short… cin-hmi… grou… owne… job_qb3.…        380       10 2017-10-25 10:46:10
10 short… cin-hmi… grou… owne… job_qb3.…        381       10 2017-10-25 10:48:42
# … with 130,000 more rows, and 36 more variables: start_time <dttm>,
#   end_time <dttm>, failed <int>, exit_status <int>, ru_wallclock <drtn>,
#   ru_utime <drtn>, ru_stime <drtn>, ru_maxrss <chr>, ru_ixrss <chr>,
#   ru_ismrss <chr>, ru_idrss <chr>, ru_isrss <chr>, ru_minflt <dbl>,
#   ru_majflt <dbl>, ru_nswap <dbl>, ru_inblock <dbl>, ru_oublock <dbl>,
#   ru_msgsnd <dbl>, ru_msgrcv <dbl>, ru_nsignals <dbl>, ru_nvcsw <dbl>,
#   ru_nivcsw <dbl>, project <chr>, department <chr>, granted_pe <chr>,
#   slots <int>, task_number <int>, cpu <drtn>, mem <chr>, io <chr>,
#   category <chr>, iow <drtn>, pe_taskid <chr>, maxvmem <dbl>, arid <dbl>,
#   ar_sub_time <dttm>
```


## Statistics on these jobs


```r
period <- range(jobs$end_time, na.rm=TRUE)
cat(sprintf("Period: %s/%s\n", period[1], period[2]))
# Period: 2017-10-24 09:40:46/2017-11-10 14:45:30

cat(sprintf("Number of jobs finished during this period: %d\n", nrow(jobs)))
# Number of jobs finished during this period: 130010

nusers <- length(unique(jobs$owner))
cat(sprintf("Number of unique users: %d\n", nusers))
# Number of unique users: 11
```

Let's see how many of the jobs finished succesfully and how many failed.

```r
## Get successful and failed jobs
groups <- list(
  success = subset(jobs, failed == 0L),
  fail    = subset(jobs, failed > 0L)
)
stats <- sapply(groups, nrow)
print(stats)
# success    fail 
#  191701    5837 

print(stats/sum(stats))
#    success       fail 
# 0.97045125 0.02954875
```

We see that failure rate among the ~200,000 jobs that ran the last 24 hours was ~3%.  Next, let's see how much CPU time this correspondsa to:

```r
## CPU time consumed
cpu <- sapply(groups, function(jobs) { d <- sum(jobs$cpu); units(d) <- "days"; d })
cat(sprintf("Total CPU processing time: %.1f hours\n", sum(cpu)))
# Total CPU processing time: 7937.3 hours

print(cpu)
#   success      fail 
# 7562.2252  375.1175

## CPU-time fractions
print(cpu / sum(cpu))
#    success       fail 
# 0.95274017 0.04725983
```

From this, we find that during the last 24 hours, ~5% of the CPU time was consumed by jobs that failed.  Among the failed jobs, the failure code was distributed as:

```r
print(table(groups$fail$failed))
#   25   26   37  100 
#    5    1 4440 1391
```

From `help("read_sge_accounting", package="wyntonquery")` for details on the parsed fields and their data types, these description of some of these codes are:

 *  25: rescheduling
 *  26: failed opening stderr/stdout file
 *  37: ran but killed because it exceeded the maximum run time (`h_rt`), maximum CPU time (`h_cpu`), or maximum virtual memory (`h_vmem`)
 * 100: ran, but killed by a signal (perhaps due to exceeding resources), task died, shepherd died (e.g. node crash), etc.


The jobs that exhausted their limits, consumed 213 days of CPU time;

```r
d <- sum(subset(groups$fail, failed == 37)$cpu); units(d) <- "days"
print(d)
# Time difference of 212.8236 days
```

which corresponds to ~3% of all CPU time spent:

```r
> as.numeric(d/sum(cpu))
[1] 0.02681296
```
